<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Deepin（Linux）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala） | 泡泡的博客</title><meta name="author" content="徐一杰"><meta name="copyright" content="徐一杰"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="安装版本jdk8、Hadoop3.2.2、Hbase2.4.17、Hive3.1.2、MySQL8.0.24、Spark3.1.1、Scala2.13.5（版本不对会报错，我下面均使用压缩包来安装对应版本） 下载所有环境Hadoop3.2.2下载 bfsu这个镜像下载最快Hbase2.4.17下载 选择bin.tar.gz Hive3.1.2下载选择bin.tar.gzSpark下载地址记得第2栏">
<meta property="og:type" content="article">
<meta property="og:title" content="Deepin（Linux）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala）">
<meta property="og:url" content="https://bubblingxuyijie.github.io/2021/04/08/Deepin%EF%BC%88Ubuntu%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83%EF%BC%88%E9%9B%86%E6%88%90Hbase%E3%80%81Hive%E3%80%81MySQL%E3%80%81Spark%E3%80%81Scala%EF%BC%89/index.html">
<meta property="og:site_name" content="泡泡的博客">
<meta property="og:description" content="安装版本jdk8、Hadoop3.2.2、Hbase2.4.17、Hive3.1.2、MySQL8.0.24、Spark3.1.1、Scala2.13.5（版本不对会报错，我下面均使用压缩包来安装对应版本） 下载所有环境Hadoop3.2.2下载 bfsu这个镜像下载最快Hbase2.4.17下载 选择bin.tar.gz Hive3.1.2下载选择bin.tar.gzSpark下载地址记得第2栏">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop0.png">
<meta property="article:published_time" content="2021-04-08T04:14:25.000Z">
<meta property="article:modified_time" content="2023-07-17T16:06:40.240Z">
<meta property="article:author" content="徐一杰">
<meta property="article:tag" content="Debian">
<meta property="article:tag" content="Ubuntu">
<meta property="article:tag" content="Deepin">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="Hbase">
<meta property="article:tag" content="Hive">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop0.png"><link rel="shortcut icon" href="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/android-chrome-192x192.png"><link rel="canonical" href="https://bubblingxuyijie.github.io/2021/04/08/Deepin%EF%BC%88Ubuntu%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83%EF%BC%88%E9%9B%86%E6%88%90Hbase%E3%80%81Hive%E3%80%81MySQL%E3%80%81Spark%E3%80%81Scala%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deepin（Linux）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-07-18 00:06:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/182382641051648891248098.jpeg" onerror="onerror=null;src='https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">68</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">88</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 工具大全</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://xuyijie.icu/"><span> 泡泡的Infinity</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://undersea.xuyijie.icu/"><span> 海里手机软件</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fireworks.xuyijie.icu/"><span> 给你看烟花</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://cloud.xuyijie.icu/"><span> 泡泡云盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://i.loli.net/2019/12/20/xb2DR3qmGutphHO.jpg"><span> 色图</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop0.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">泡泡的博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 工具大全</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://xuyijie.icu/"><span> 泡泡的Infinity</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://undersea.xuyijie.icu/"><span> 海里手机软件</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fireworks.xuyijie.icu/"><span> 给你看烟花</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://cloud.xuyijie.icu/"><span> 泡泡云盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://i.loli.net/2019/12/20/xb2DR3qmGutphHO.jpg"><span> 色图</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deepin（Linux）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-04-08T04:14:25.000Z" title="发表于 2021-04-08 12:14:25">2021-04-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-07-17T16:06:40.240Z" title="更新于 2023-07-18 00:06:40">2023-07-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Linux/">Linux</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Deepin（Linux）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2021/04/08/Deepin%EF%BC%88Ubuntu%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83%EF%BC%88%E9%9B%86%E6%88%90Hbase%E3%80%81Hive%E3%80%81MySQL%E3%80%81Spark%E3%80%81Scala%EF%BC%89/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2021/04/08/Deepin%EF%BC%88Ubuntu%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83%EF%BC%88%E9%9B%86%E6%88%90Hbase%E3%80%81Hive%E3%80%81MySQL%E3%80%81Spark%E3%80%81Scala%EF%BC%89/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="安装版本"><a href="#安装版本" class="headerlink" title="安装版本"></a>安装版本</h1><p>jdk8、Hadoop3.2.2、Hbase2.4.17、Hive3.1.2、MySQL8.0.24、Spark3.1.1、Scala2.13.5<br>（版本不对会报错，我下面均使用压缩包来安装对应版本）</p>
<h1 id="下载所有环境"><a href="#下载所有环境" class="headerlink" title="下载所有环境"></a>下载所有环境</h1><p><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/">Hadoop3.2.2下载</a> bfsu这个镜像下载最快<img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/HadoopDownload.png" alt="在这里插入图片描述"><br><a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hbase/2.4.17/">Hbase2.4.17下载</a> 选择bin.tar.gz</p>
<p><a target="_blank" rel="noopener" href="https://mirrors.bfsu.edu.cn/apache/hive/hive-3.1.2/">Hive3.1.2下载</a>选择bin.tar.gz<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop1.png" alt="在这里插入图片描述"><br><a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">Spark下载地址</a>记得第2栏选择Hadoop版本，本教程是3.2<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop2.png" alt="在这里插入图片描述"><br><a target="_blank" rel="noopener" href="https://www.scala-lang.org/download/">Scala2.13.5下载地址</a> 进去官网后拉到最底部，选择第一个<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop3.png" alt="在这里插入图片描述"></p>
<p><a target="_blank" rel="noopener" href="https://dev.mysql.com/downloads/mysql/">MySQL下载地址</a></p>
<p>这里一定要选择这个Linux Generic<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop4.png" alt="在这里插入图片描述"><br>下载前面这两个，64位和32位根据自己电脑情况来选</p>
<h1 id="开启Deepin或Ubuntu（我用的是虚拟机）"><a href="#开启Deepin或Ubuntu（我用的是虚拟机）" class="headerlink" title="开启Deepin或Ubuntu（我用的是虚拟机）"></a>开启Deepin或Ubuntu（我用的是虚拟机）</h1><p>我用的是Deepin20（Debian10 Buster库）好看吧， 还有“QQ2008”<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop5.png" alt="在这里插入图片描述"></p>
<hr>
<h1 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h1><h4 id="1、安装和配置ssh"><a href="#1、安装和配置ssh" class="headerlink" title="1、安装和配置ssh"></a>1、安装和配置ssh</h4><p>首先在终端输入sudo apt update来更新一下apt的包列表（apt代表赋予管理员权限，建议每句命令都加上）<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop6.png" alt="在这里插入图片描述"><br>输入sudo apt-get install openssh-server回车，再输入sudo apt-get install openssh-client回车（Deepin可能已经预装了ssh，不过再输入一下确认一下也没什么）<br>Ubuntu安装是如下界面<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop7.png" alt="在这里插入图片描述"><br>Deepin已经安装过是如下界面<img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop8.png" alt="在这里插入图片描述"><br>配置ssh无密码自动登录（很重要）</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> ssh localhost              <span class="comment">#登陆SSH，第一次登陆输入yes</span></span><br></pre></td></tr></table></figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="keyword">exit</span>                       <span class="comment">#退出登录的ssh localhost</span></span><br></pre></td></tr></table></figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="built_in">cd</span> ~/.ssh/                 <span class="comment">#如果没法进入该目录，执行一次ssh localhost</span></span><br></pre></td></tr></table></figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> ssh<span class="literal">-keygen</span> <span class="literal">-t</span> rsa</span><br></pre></td></tr></table></figure>

<p>输入完$ ssh-keygen -t rsa 语句后，需要连续敲击三次回车<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop9.png" alt="在这里插入图片描述"></p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="built_in">cat</span> ./id_rsa.pub &gt;&gt; ./authorized_keys <span class="comment">#加入授权</span></span><br></pre></td></tr></table></figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> ssh localhost     <span class="comment">#此时已不需密码即可登录localhost，并可见下图的Welcome</span></span><br></pre></td></tr></table></figure>
<p><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop10.png" alt="在这里插入图片描述"></p>
<h4 id="2、安装和配置Java（一定要安装Java8版本，不然Hive、Spark和Scala会报错）"><a href="#2、安装和配置Java（一定要安装Java8版本，不然Hive、Spark和Scala会报错）" class="headerlink" title="2、安装和配置Java（一定要安装Java8版本，不然Hive、Spark和Scala会报错）"></a>2、安装和配置Java（一定要安装Java8版本，不然Hive、Spark和Scala会报错）</h4><p>输入apt-cache search openjdk ,这是查看可以安装的Java版本有哪些，查询结果显示如下，我们可以看到，又openjdk-8-jdk<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop11.png" alt="在这里插入图片描述"><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop47.png" alt="在这里插入图片描述"><br>接着，我们输入sudo apt-get install openjdk-8-jdk 回车，等待下载就安装成功了</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> sudo apt<span class="literal">-get</span> install openjdk<span class="literal">-8-jdk</span></span><br></pre></td></tr></table></figure>

<p>无论是Deepin还是Ubuntu，apt安装的Java都存在系统盘的usr&#x2F;lib.jvm里<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop12.png" alt="在这里插入图片描述"><br>最后配置Java的环境变量</p>
<p>我们在终端中输入vim ~&#x2F;.bashrc ,会进入一个文档，也许会出现这个页面，按一下键盘上的E就能进入文档了</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> vim ~/.bashrc</span><br></pre></td></tr></table></figure>

<p><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop13.png" alt="在这里插入图片描述"><br>进入文档后是这样的<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop14.png" alt="在这里插入图片描述"><br>我这边配置了本教程所有的环境变量，大家之间粘贴进去，后面就不用再配置了</p>
<p>注意！！！vim的操作和普通文本编辑器不同！！！</p>
<p>vim进去以后，先按一下键盘上的A，进入编辑模式，然后再右键，点粘贴，或者是CTRL+SHIFT+V粘贴，粘贴上去以后，按一下键盘上的“ESC”键，然后按“SHIFT+ZZ”就保存并退出到终端了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk8</span><br><span class="line"><span class="built_in">export</span> JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib:<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin:<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$CALSSPATH</span></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/local/scala</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/local/hive</span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/usr/local/hbase</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin:<span class="variable">$HBASE_HOME</span>/bin:<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$JRE_HOME</span>/bin:<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$SCALA_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin</span><br></pre></td></tr></table></figure>

<p>然后输入source ~&#x2F;.bashrc，这个意思是使配置的环境变量立即生效<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop15.png" alt="在这里插入图片描述"><br>检查Java版本，安装并配置成功<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop16.png" alt="在这里插入图片描述"></p>
<hr>
<h4 id="3、安装Hadoop"><a href="#3、安装Hadoop" class="headerlink" title="3、安装Hadoop"></a>3、安装Hadoop</h4><p>我们把下载好的Hadoop安装包放在“下载”文件夹里，Deepin下载的默认路径就是这个</p>
<p>在这个目录下右键选择“在终端中打开”<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop17.png" alt="在这里插入图片描述"><br>在打开的终端中输入sudo tar -zxvf  hadoop-3.2.2.tar -C &#x2F;usr&#x2F;local</p>
<p>将下载的Hadoop压缩包解压到usr&#x2F;local&#x2F;hadoop-3.2.2文件夹下，hadoop-3.2.2.tar.gz是下载的文件名</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar <span class="literal">-zxvf</span>  hadoop<span class="literal">-3</span>.<span class="number">2.2</span>.tar.gz <span class="literal">-C</span> /usr/local</span><br></pre></td></tr></table></figure>
<p><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop18.png" alt="在这里插入图片描述"><br>然后将hadoop-3.2.2重命名为hadoop，依次执行下列命令</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> <span class="built_in">cd</span> /usr/local</span><br></pre></td></tr></table></figure>

<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> sudo <span class="built_in">mv</span> ./hadoop<span class="literal">-3</span>.<span class="number">2.2</span> ./hadoop</span><br></pre></td></tr></table></figure>

<p>查看文件夹，已经命名为hadoop<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop19.png" alt="在这里插入图片描述"><br>你们的hadoop文件夹上肯定会带有一个黄色的锁🔒，还需要赋予hadoop文件夹权限，避免以后出现问题，接着执行下列命令，🔒就消失了，以后安装Hbase、Hive等，也是执行下列命令赋予权限，把文件夹名修改一下就行</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> sudo chmod <span class="number">777</span> <span class="literal">-R</span> hadoop</span><br></pre></td></tr></table></figure>

<p>伪分布需要配置三个文件</p>
<p>1、&#x2F;hadoop&#x2F;etc&#x2F;hadoop路径下的hadoop-env.sh，右键打开方式选文本编辑器，添加下面代码进去<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop20.png" alt="在这里插入图片描述"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_OS_TYPE=<span class="variable">$&#123;HADOOP_OS_TYPE:-$(uname -s)&#125;</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk8</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_SSH_OPTS=<span class="string">&quot;-p 22&quot;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=.:<span class="variable">$CLASSPATH</span>:<span class="variable">$HADOOP_CLASSPATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop21.png" alt="在这里插入图片描述"><br>2、&#x2F;hadoop&#x2F;etc&#x2F;hadoop路径下的core-site.xml，添加下面代码进去</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>HDFS的URI，文件系统://namenode标识:端口号<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上本地的hadoop临时文件夹<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3、&#x2F;hadoop&#x2F;etc&#x2F;hadoop路径下的hdfs-site.xml，添加下面代码进去</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode上存储hdfs名字空间元数据 <span class="tag">&lt;/<span class="name">description</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode上数据块的物理存储位置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/snn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>secondary namenode 的位置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.checkpoint.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/snn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>secondary namenode 的位置<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>副本个数，配置默认是3,应小于datanode机器数量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>Hadoop 的运行方式是由配置文件决定的，因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（参考官方教程），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 &#x2F;tmp&#x2F;hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p>
<p>配置完之后，执行根节点的格式化，在hadoop文件夹下右键，“在终端中打开”<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop22.png" alt="在这里插入图片描述"></p>
<p>输入bin&#x2F;hdfs namenode -format<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop23.png" alt="在这里插入图片描述"><br>格式化根节点只在第一次配置完之后执行一次，切勿多次手动格式化，如果多次手动格式化导致DataNode或NameNode无法启动，请关闭Hadoop服务，删除hadoop文件夹下的这三个文件夹后，重新格式化根节点<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop24.png" alt="在这里插入图片描述"></p>
<h4 id="4、启动Hadoop"><a href="#4、启动Hadoop" class="headerlink" title="4、启动Hadoop"></a>4、启动Hadoop</h4><p>我们已经配置好环境变量，所以可以在终端中直接输入start-dfs.sh来启动Hadoop的服务<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop25.png" alt="在这里插入图片描述"><br>注意！！！！今后启动不再使用start-dfs.sh命令，因为这个命令没有启动所有的Hadoop服务，以后做项目或者配置其他环境会报错，记住，以后启动Hadoop使用start-all.sh这个命令</p>
<p>打开浏览器，输入localhost:9870（Hadoop3的默认端口）<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop26.png" alt="在这里插入图片描述"></p>
<p>关闭Hadoop服务的命令是stop-dfs.sh</p>
<p>如果启动出现如下报错</p>
<p><code>ERROR: Attempting to operate on hdfs namenode as root</code></p>
<p><code>ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.</code></p>
<p>修改hadoop&#x2F;sbin下面的start-dfs.sh和stop-dfs.sh，添加内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">HDFS_ZKFC_USER=root</span><br><span class="line">HDFS_JOURNALNODE_USER=root</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HDFS_DATANODE_SECURE_USER=root</span><br></pre></td></tr></table></figure>

<p>修改start-yarn.sh和stop-yarn.sh，添加内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HDFS_DATANODE_SECURE_USER=root</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="安装Hbase"><a href="#安装Hbase" class="headerlink" title="安装Hbase"></a>安装Hbase</h1><h4 id="1、解压Hbase"><a href="#1、解压Hbase" class="headerlink" title="1、解压Hbase"></a>1、解压Hbase</h4><p>参考解压hadoop进行解压<br>重命名文件夹<br>赋予权限</p>
<p>本教程安装的所有的环境都解压在&#x2F;usr&#x2F;local里，后面不在赘述解压过程<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop27.png" alt="在这里插入图片描述"></p>
<h4 id="2、配置Hbase伪分布环境"><a href="#2、配置Hbase伪分布环境" class="headerlink" title="2、配置Hbase伪分布环境"></a>2、配置Hbase伪分布环境</h4><p>将下面代码复制到Hbase文件夹里的conf文件夹里的hbase-site.xml文件中</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 端口要和Hadoop的fs.defaultFS端口一致--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 是否分布式部署 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2182<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                                                                                                                                             </span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60010<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                                                                                                                                             </span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>将下面代码复制到Hbase文件夹里的conf文件夹里的hbase-env.sh文件中</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk8</span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br><span class="line">export HBASE_CLASSPATH=/usr/local/hbase/conf</span><br><span class="line">export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP=&quot;true&quot;</span><br></pre></td></tr></table></figure>

<p>然后就可以启动Hbase了，注意，启动Hbase之前先启动Hadoop，关闭Hbase的命令是stop-hbase.sh<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop28.png" alt="在这里插入图片描述"><br>可以localhost:60010访问Hbase页面<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop29.png" alt="在这里插入图片描述"></p>
<h4 id="3、hbase-shell的基本操作"><a href="#3、hbase-shell的基本操作" class="headerlink" title="3、hbase shell的基本操作"></a>3、hbase shell的基本操作</h4><p>输入hbase shell可以打开hbase shell<br>建个表<br>查看表属性<br>添加一个学生信息<br>查看学生信息<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop30.png" alt="在这里插入图片描述"></p>
<p><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop31.png" alt="在这里插入图片描述"></p>
<p>退出shell输入exit回车</p>
<hr>
<h1 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a>安装Hive</h1><h4 id="1、安装Hive"><a href="#1、安装Hive" class="headerlink" title="1、安装Hive"></a>1、安装Hive</h4><p>解压过程不赘述，请看安装Haoop板块介绍的方法</p>
<p>修改Hive&#x2F;conf文件夹下的hive-site.xml，如果没有就在该文件夹下右键，“在终端中打开”，使用下面命令新建一个hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo gedit hive-site.xml</span><br></pre></td></tr></table></figure>
<p>或者直接右键，新建文本文档，后缀修改为xml<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop32.png" alt="在这里插入图片描述"><br>在hive-site.xml中这样写，此配置为MySQL8.0.24的配置，如果版本低于8，则不是这样配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span> standalone=<span class="string">&quot;no&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 日志目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/log<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置metastore的节点信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://linux100:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 客户端远程连接的端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.webui.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hive服务的页面的端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.webui.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10002<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.long.polling.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                               </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.autoCreateSchema<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.fixedDatastore<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mr<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置 --&gt;</span></span><br><span class="line">	 <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改hive&#x2F;conf文件夹下的hive-env.sh，添加下面语句</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop/</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/local/hive/conf/</span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/usr/local/hive/lib</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="2、安装MySQL"><a href="#2、安装MySQL" class="headerlink" title="2、安装MySQL"></a>2、安装MySQL</h4><p>MySQL可以使用apt安装，Deepin默认的MySQL就是最新版，Ubuntu我不知道是不是，可以使用下面命令看一下版本<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop33.png" alt="在这里插入图片描述"><br>然后使用下面命令安装MySQL</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt<span class="literal">-get</span> install mysql<span class="literal">-server</span></span><br></pre></td></tr></table></figure>
<p>安装完成后查看MySQL版本<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop34.png" alt="在这里插入图片描述"><br>接下来还要安装一个mysql-connector-java<br><a target="_blank" rel="noopener" href="https://dev.mysql.com/downloads/connector/j/">下载地址</a><br>Deepin和UOS选Debian，Ubuntu选Ubuntu<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop35.png" alt="在这里插入图片描述"><br>下载完成后，文件是deb格式，直接双击点击安装就行了，我的因为安装过了，只有更新可以点<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop36.png" alt="在这里插入图片描述"><br>在终端输入以下命令启动MySQL服务</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">start</span> mysql</span><br></pre></td></tr></table></figure>
<p>执行sudo mysql_secure_installation</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>为MySQL设置密码（一定要设置，而且密码不能为空，不然后面报错）<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop37.png" alt="在这里插入图片描述"><br>然后下面会让你输入很多Y&#x2F;N，全部输入Y回车</p>
<p>完成所有设置后，以root用户登录MySQL。 在终端中，键入以下命令：mysql -u root -p，输入 root用户的密码，然后按Enter ，登陆进MySQL<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop38.png" alt="在这里插入图片描述"><br>新建hive数据库，别忘了加分号<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop39.png" alt="在这里插入图片描述"><br>然后依次输入下面语句，意思是将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;hive&#x27;</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">ON</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>然后输入下面语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flush privileges;  #刷新mysql系统权限关系表</span><br></pre></td></tr></table></figure>
<p>再新建一个数据库叫hive<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop40.png" alt="在这里插入图片描述"></p>
<h4 id="3、启动Hive"><a href="#3、启动Hive" class="headerlink" title="3、启动Hive"></a>3、启动Hive</h4><p>启动Hive之前，先运行start-all.sh启动Hadoop集群，然后输入hive启动</p>
<p>如果输入hive以后出现下面报错 ，则输入hdfs dfsadmin -safemode leave关闭安全模式，再运行hive即可<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop41.png" alt="在这里插入图片描述"><br>启动成功，输入exitl;可退出hive shell<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop42.png" alt="在这里插入图片描述"></p>
<hr>
<h1 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h1><h4 id="1、安装Scala语言支持"><a href="#1、安装Scala语言支持" class="headerlink" title="1、安装Scala语言支持"></a>1、安装Scala语言支持</h4><p>解压不再赘述，请看安装Haoop板块介绍的方法，安装好查看版本<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop43.png" alt="在这里插入图片描述"></p>
<h4 id="2、安装Spark"><a href="#2、安装Spark" class="headerlink" title="2、安装Spark"></a>2、安装Spark</h4><p>配置spark&#x2F;conf文件夹下的spark-env.sh</p>
<p>在最后一行下面添加</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk8</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/local/scala</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/local/spark</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=127.0.0.1</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_WEBUI_PORT=8099</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_CORES=3</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_INSTANCES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_MEMORY=5G</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_CORES=1</span><br><span class="line"><span class="built_in">export</span> SPARK_EXECUTOR_MEMORY=1G</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$HADOOP_HOME</span>/lib/native</span><br></pre></td></tr></table></figure>
<p>配置spark&#x2F;conf文件夹下的worker，就在最后一行下面加上一个localhost即可（Spark基于hadoop3.2版本的配置文件从slaves变成了worker，旧版本为slaves）<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop44.png" alt="在这里插入图片描述"></p>
<h4 id="3、启动Spark"><a href="#3、启动Spark" class="headerlink" title="3、启动Spark"></a>3、启动Spark</h4><p>启动Spark之前，同样要先保证Hadoop集群已经启动</p>
<p>启动Spark使用start-master.sh和start-slaves.sh</p>
<p>然后输入jps，查看，多出了Master和Worker<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop45.png" alt="在这里插入图片描述"></p>
<h4 id="4、开启Spark-Shell"><a href="#4、开启Spark-Shell" class="headerlink" title="4、开启Spark Shell"></a>4、开启Spark Shell</h4><p>spark shell的退出命令是 :quit<br><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop46.png" alt="在这里插入图片描述"></p>
<hr>
<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>4.10：好了这篇文章到这就暂时结束了，Linux真sd，这系非人类，虽然早就配好了，但是等我心情好了再写吧<br>4.29：心情不错，正在更新<br>4.29：一口气写完</p>
<p>后续会写一个WordCount实例的教程</p>
<p>WordCount实例教程已写完，项目打包成jar包，可以在hadoop下运行</p>
<p>传送门：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_48922459/article/details/116274365?spm=1001.2014.3001.5501">MapReduce基础编程，实现WordCount实例</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://bubblingxuyijie.github.io">徐一杰</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://bubblingxuyijie.github.io/2021/04/08/Deepin%EF%BC%88Ubuntu%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83%EF%BC%88%E9%9B%86%E6%88%90Hbase%E3%80%81Hive%E3%80%81MySQL%E3%80%81Spark%E3%80%81Scala%EF%BC%89/">https://bubblingxuyijie.github.io/2021/04/08/Deepin（Ubuntu通用）安装Hadoop伪分布环境（集成Hbase、Hive、MySQL、Spark、Scala）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://bubblingxuyijie.github.io" target="_blank">泡泡的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Debian/">Debian</a><a class="post-meta__tags" href="/tags/Ubuntu/">Ubuntu</a><a class="post-meta__tags" href="/tags/Deepin/">Deepin</a><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/Hbase/">Hbase</a><a class="post-meta__tags" href="/tags/Hive/">Hive</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/DebianHadoop0.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/MyWeChatPay.png" target="_blank"><img class="post-qr-code-img" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/MyWeChatPay.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/MyAliPay.jpg" target="_blank"><img class="post-qr-code-img" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/MyAliPay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/04/29/MapReduce%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%EF%BC%8CHadoop%E5%AE%9E%E7%8E%B0WordCount%E5%AE%9E%E4%BE%8B%EF%BC%88%E6%89%93%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E8%A1%8C%EF%BC%89/"><img class="prev-cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/MapReduce0.png" onerror="onerror=null;src='https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MapReduce基础编程，Hadoop实现WordCount实例（打包上传到Linux服务器运行）</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/19/%E6%81%92%E6%98%9F%E7%9A%84%E6%BC%94%E5%8C%96%E5%92%8C%E5%87%A0%E7%A7%8D%E6%9C%80%E7%BB%88%E5%BD%92%E5%AE%BF%EF%BC%88%E5%8C%85%E6%8B%AC%E9%BB%91%E6%B4%9E%E3%80%81%E8%B6%85%E6%96%B0%E6%98%9F%E7%AD%89%E7%89%B9%E6%AE%8A%E5%A4%A9%E4%BD%93%EF%BC%89/"><img class="next-cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/恒星演化0.png" onerror="onerror=null;src='https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">恒星的演化和几种最终归宿（包括黑洞、超新星等特殊天体）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/28/Debian11%EF%BC%88Linux%20%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85%20Jenkins%20%E5%B9%B6%E9%85%8D%E7%BD%AE%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/" title="Debian11（Linux 通用）安装 Jenkins 并配置自动化部署"><img class="cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/Jenkins18.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-28</div><div class="title">Debian11（Linux 通用）安装 Jenkins 并配置自动化部署</div></div></a></div><div><a href="/2022/05/09/Debian(Linux)%20%E5%AE%89%E8%A3%85Windows%E9%80%9A%E7%94%A8%E5%AD%97%E4%BD%93(%E5%8F%AF%E8%A7%A3%E5%86%B3TimesNewRoman%E7%AD%89%E5%AD%97%E4%BD%93%E7%9A%84%E6%8A%A5%E9%94%99)/" title="Debian(Linux) 安装Windows通用字体(可解决TimesNewRoman等字体的报错)"><img class="cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/LinuxLogo.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-09</div><div class="title">Debian(Linux) 安装Windows通用字体(可解决TimesNewRoman等字体的报错)</div></div></a></div><div><a href="/2022/11/01/Debian%EF%BC%88Linux%E9%80%9A%E7%94%A8%EF%BC%89%E5%AE%89%E8%A3%85%20Kafka%20%E5%B9%B6%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/" title="Debian（Linux通用）安装 Kafka 并配置远程访问"><img class="cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/Kafka.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-01</div><div class="title">Debian（Linux通用）安装 Kafka 并配置远程访问</div></div></a></div><div><a href="/2021/04/29/MapReduce%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%EF%BC%8CHadoop%E5%AE%9E%E7%8E%B0WordCount%E5%AE%9E%E4%BE%8B%EF%BC%88%E6%89%93%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E8%A1%8C%EF%BC%89/" title="MapReduce基础编程，Hadoop实现WordCount实例（打包上传到Linux服务器运行）"><img class="cover" src="https://qiniuoss.xuyijie.icu/XuYijieBlog/BlogImage/MapReduce0.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-29</div><div class="title">MapReduce基础编程，Hadoop实现WordCount实例（打包上传到Linux服务器运行）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/182382641051648891248098.jpeg" onerror="this.onerror=null;this.src='https://qiniuoss.xuyijie.icu/XuYijieBlog/ThemeImage/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">徐一杰</div><div class="author-info__description">编程技术、代码资源、各类技术、各种工具、影视资源分享</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">68</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">88</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://xuyijie.icu"><i></i><span>泡泡的Infinity</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/BubblingXuYijie" target="_blank" title="GITHUB"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_48922459" target="_blank" title="CSDN"><i class="fa-solid fa-blog"></i></a><a class="social-icon" href="mailto:1119461672@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://t.me/xuyijie" target="_blank" title="Telegram"><i class="fa-brands fa-telegram"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1119461672&amp;website=www.oicqzone.com" target="_blank" title="QQ"><i class="fa-brands fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">博文和本人的CSDN同步，有问题可以点击右下角图标和我聊天或者评论文章哦。（Blogs is synchronized with my CSDN.If you have any questions, you can click the lower right gear to chat with me or comment on the article.）</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%89%88%E6%9C%AC"><span class="toc-number">1.</span> <span class="toc-text">安装版本</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%89%80%E6%9C%89%E7%8E%AF%E5%A2%83"><span class="toc-number">2.</span> <span class="toc-text">下载所有环境</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%80%E5%90%AFDeepin%E6%88%96Ubuntu%EF%BC%88%E6%88%91%E7%94%A8%E7%9A%84%E6%98%AF%E8%99%9A%E6%8B%9F%E6%9C%BA%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">开启Deepin或Ubuntu（我用的是虚拟机）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hadoop"><span class="toc-number">4.</span> <span class="toc-text">安装Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEssh"><span class="toc-number">4.0.0.1.</span> <span class="toc-text">1、安装和配置ssh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AEJava%EF%BC%88%E4%B8%80%E5%AE%9A%E8%A6%81%E5%AE%89%E8%A3%85Java8%E7%89%88%E6%9C%AC%EF%BC%8C%E4%B8%8D%E7%84%B6Hive%E3%80%81Spark%E5%92%8CScala%E4%BC%9A%E6%8A%A5%E9%94%99%EF%BC%89"><span class="toc-number">4.0.0.2.</span> <span class="toc-text">2、安装和配置Java（一定要安装Java8版本，不然Hive、Spark和Scala会报错）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E5%AE%89%E8%A3%85Hadoop"><span class="toc-number">4.0.0.3.</span> <span class="toc-text">3、安装Hadoop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E5%90%AF%E5%8A%A8Hadoop"><span class="toc-number">4.0.0.4.</span> <span class="toc-text">4、启动Hadoop</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hbase"><span class="toc-number">5.</span> <span class="toc-text">安装Hbase</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E8%A7%A3%E5%8E%8BHbase"><span class="toc-number">5.0.0.1.</span> <span class="toc-text">1、解压Hbase</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E9%85%8D%E7%BD%AEHbase%E4%BC%AA%E5%88%86%E5%B8%83%E7%8E%AF%E5%A2%83"><span class="toc-number">5.0.0.2.</span> <span class="toc-text">2、配置Hbase伪分布环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81hbase-shell%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">5.0.0.3.</span> <span class="toc-text">3、hbase shell的基本操作</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hive"><span class="toc-number">6.</span> <span class="toc-text">安装Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%AE%89%E8%A3%85Hive"><span class="toc-number">6.0.0.1.</span> <span class="toc-text">1、安装Hive</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%AE%89%E8%A3%85MySQL"><span class="toc-number">6.0.0.2.</span> <span class="toc-text">2、安装MySQL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E5%90%AF%E5%8A%A8Hive"><span class="toc-number">6.0.0.3.</span> <span class="toc-text">3、启动Hive</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Spark"><span class="toc-number">7.</span> <span class="toc-text">安装Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E5%AE%89%E8%A3%85Scala%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81"><span class="toc-number">7.0.0.1.</span> <span class="toc-text">1、安装Scala语言支持</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%AE%89%E8%A3%85Spark"><span class="toc-number">7.0.0.2.</span> <span class="toc-text">2、安装Spark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E5%90%AF%E5%8A%A8Spark"><span class="toc-number">7.0.0.3.</span> <span class="toc-text">3、启动Spark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81%E5%BC%80%E5%90%AFSpark-Shell"><span class="toc-number">7.0.0.4.</span> <span class="toc-text">4、开启Spark Shell</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">8.</span> <span class="toc-text">结语</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2023 By 徐一杰</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">很开心见到你ヾ(^▽^*)))</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'hkhwUbLes3CPVXrOaA5GWlEC-9Nh9j0Va',
      appKey: 'yjP1tNi7HQcdN3abvEzimV0Y',
      avatar: 'monsterid',
      serverURLs: 'https://leancloud.xuyijie.icu',
      emojiMaps: {"tv_doge":"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_亲亲":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再见":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_发怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_发财":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可爱":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_呕吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_坏笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尴尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_惊吓":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"},
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script src="//code.tidio.co/9rfawxoiu99lc40ynelt5vvxgkhickux.js" async="async"></script><script>function onTidioChatApiReady() {
  window.tidioChatApi.hide();
  window.tidioChatApi.on("close", function() {
    window.tidioChatApi.hide();
  });
}
if (window.tidioChatApi) {
  window.tidioChatApi.on("ready", onTidioChatApiReady);
} else {
  document.addEventListener("tidioChat-ready", onTidioChatApiReady);
}

var chatBtnFn = () => {
  document.getElementById("chat_btn").addEventListener("click", function(){
    window.tidioChatApi.show();
    window.tidioChatApi.open();
  });
}
chatBtnFn()
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>